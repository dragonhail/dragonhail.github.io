---
title: 관측가능성
weight: 13
---
## 참고지식
#### 블랙박스 테스트
- 내부 구조 확인하지 않고 테스트(Jenkins의 테스트도 해당)
- equivalence partitioning - 동등값 분할
  - 성공하는게 10번이면 실패도 10번
  - 실패에 대해서도 테스트 해야 함
- boundary value analysis - 경계값 분석
  - 논리적 오류는 경계에서 많이 발생
- Error Guessing - 오류 예측
  - 경험으로 오류를 아는 것
- cause ~ Effect Graph: 원인 결과 그래프
- 의사 결정 테이블
- 상태 정의

#### 알파, 베타 테스트
- 알파: 개발자 환경에서 테스트
- 베타: 사용자 환경에서 테스트

#### 화이트박스 테스트
- 내부 구조 확인하는 것
- 문장 검증
- 분기 검증
- 조건 검증
- 경로 검증

### C, go VS java, C#
- C, go: 실행에 필요한 모든 것을 빌드할 때 포함
  - go는 쓰지 않는 라이브러리를 import에 추가시키면 에러가 발생
- Java, C#: 실행에 필요한 라이브러리나 일부를 VM이나 프레임워크가 소유

### 로드밸런서와 API 게이트웨이
- API 게이트웨이 인스턴스들 앞에 로드밸런서를 부착
```
플랫폼 로드밸런서 -> API Gateway1, API Gateway2.. -> 서비스1, 서비스2, 서비스1, 서비스2 ...
```

### 가용영역을 서비스마다 다르게 하는 이유
- 클러스터
  - Cluster > VPC > 서브넷(논리적 개념) - AZ(가용영역을 서브넷마다 다르게 줘야 함) -> 복원력 때문
  - 3개의 서브넷 만들고 replicas는 2를 만듬
---
## 1.관측 가능성의 개념
### 1)관측 가능성의 세 가지 요소
- 모니터링 과의 차이점
  - 관측 가능성: 시스템에서 외부로 출력되는 값만을 이용해서 시스템 내부 상태를 이해하고 예측하는 것
  - 모니터링은 블랙박스 테스트(기능 테스트: Input 이 주어지면 정확한 Output이 나오는 것을 확인하는 테스트) 와 화이트박스 테스트 영역을 포함하지만 관측 가능성은 화이트박스 테스트 와 예측을 포함
  - 관측 가능성의 목표는 내부 시스템에 대한 자세한 이해를 기반으로 미래에 발생할 이벤트를 예측하는 것이고 이러한 예측을 바탕으로 IT운영을 자동화(장애가 발생할 위험이 있으면 미리 예측하고 운영자에게 통지하거나 서비스에 필요한 리소스를 증감하는 것)
  - Elastic Search를 이용해서 로그를 관리하거나 Dynatrace APM을 이용해서 처리량과 지연 시간을 모니터링하고 Influx DB와 Telegraph 등을 이용해서 Metric을 수집하고 측정하는 것들도 전부 관측 가능성의 일부
  - 관측 가능성의 특징   
    화이트박스 모니터링을 포함하며 블랙박스 모니터링은 제외

    관측 가능성은 애플리케이션 내부의 상태를 디버깅할 수 있는 정보를 제공하므로 장애가 발생한 경우 신속하고 수월하게 대응할 수 있음

    관측 가능성은 제공된 결과를 보고 이해하는 수준에 그치지 않고 원하는 계측을 추가할 수 있어야 함
  - 관측 가능성은 클라우드 네이티브처럼 분산되고 복잡한 시스템에서 발생하는 이벤트에 대한 통찰력 그리고 태그, 로그 등을 결합해서 마이크로 서비스에 대한 Context 정보를 제공하는 것이 목표
- 관측 가능성의 3가지 요소는 Metric, Log, Trace 
- 관측 가능성의 구성 요소
  - Metric   
    일정 시간 동안 측정된 데이터를 집계하고 수치화

    대표적인 것으로 Queue 의 대기 중인 메시지 개수, 사용 중인 CPU 와 Memory 크기, 서비스에서 초당 처리하는 개수 등

	Metric은 전체적인 시스템 상태를 보고하는데 유용

	대부분의 경우 히스토그램이나 게이지 등 차트를 이용해서 시각적으로 표현

    애플리케이션에서 기본으로 제공하는 Metric 이외에 Custom Metric이 필요한 경우가 있는데 이를 위해서는 계측 관련 API 나 SDK를 이용해서 개발할 수 있지만 이는 어려운 작업인데 패키지 소프트웨어 등을 이용해서 사용하는 경우가 많습니다.
  - log   
    애플리케이션 실행 시 생성되는 텍스트 라인으로 구조적인 JSON이나 비 구조적인 텍스트 형식으로 출력

    애플리케이션 에러 와 경고를 확인하고 문제점에 대한 정확한 원인을 이해하기 위해서 필요

    집계 와 알람 중심의 Metric이 지원해주지 못하는 세부적인 정보를 제공

    레거시 시스템 과 패키지 애플리케이션 일수록 Metric 보다는 Log를 사용해서 시스템 내부를 이해하고 문제 해결하는 것이 일반적

  - tracing(추적)   
    마이크로 서비스는 하나의 애플리케이션 모든 처리를 하기도 하지만 대부분의 경우는 시스템을 경유하면 트랜잭션을 처리하기 때문에 처리하는 과정에서 발생하는 세부적인 정보를 출력할 필요가 있음

    트랜잭션이 이동하는 경로, 트랜잭션을 처리하는 과정하는 발생하는 대기 시간 그리고 지연 시간, 병목 현상이나 에러를 발생시키는 원인을 Context 그리고 로그, 태그 등의 메타데이터에 출력하는 것

### 2)Metric
- 가용성
  - 시스템의 전반적인 상태와 정상 작동 여부를 거시적 관점에서 측정하고 SLI(Service Level Indicator)로 정량화
  - SLI는 SLO(Service Level Objective - 서비스 수준 목표)라는 임계 기준을 달성해야 하고 SLI의 상한 과 하한을 설정해주어야 합니다.
  - SLO는 사업적으로 합의한 수준 또는 SLA(Service Level Agreement)에 명시된 수준보다 제한적이거나 보수적인 수치
  - SLA를 위반한 위험이 생겼을 때 사전 경고를 제공하고 SLA를 위반하는 상태에 이르지 않도록 방지하는 것이 핵심
- 구글의 골든 시그널
  - 지연 과 에러
  - 구글에서는 주로 네트워크에서 발생하는 이벤트와 관련된 가이드 라인을 제시했는데 이 때 사용한 Metric 이 지연, 에러, 트래픽, 포화 네가지
  - 지연   
    시스템이 정상이 아니라서 응답을 느리게 하는 것으로 잠재적인 장애를 예측할 수 있는 신호   
    Prometheus에서는 PromQL로 지연을 계산할 수 있음
  - 에러   
    물리적 에러 와 논리적 에러가 있는데 어떤 경우를 에러로 판정할 지 기준을 명확히 설정해야
  - 트래픽   
    발생하는 요청의 양   
    시스템의 유형이나 초당 요청의 수, 네트워크 입출력 등에 따라 다양   
    지연과 에러가 발생하더라도 중단없이 적정 수준의 트래픽을 처리할 수 있는 시스템을 구축하는 것이 클라우드 네이트의 사상
  - 포화   
    어느 정도 사용하고 있는지에 대한 비율

- 메트릭 유형
  - Counter   
    모니터링하는 이벤트의 누적 개수 또는 크기를 표현   
    rate 함수 와 함께 이벤트를 추적하는 많이 사용   
    초당 요청 개수 또는 초당 처리 개수 등에 사용   
    증감할 수 있는 경우는 카운터를 사용하면 안됨
  - Gauge   
    증가하거나 감소하는 임의 값을 나타내는 Metric   
    데이터베이스에 연결된 커넥터 개수, 현재 사용하는 스레드 개수, 사용률 등에 사용
  - Summary   
    측정한 이벤트의 합계와 카운트를 같이 제공   
    범위와 분포에 상관없이 정확한 백분위수가 필요할 때 사용
  - Histogram   
    추세 와 데이터가 단일 범주에서 어떻게 분포되어 있는지를 나타냄   
    분포를 이해할 때는 분위 수 또는 표준 편차 그리고 그래프 등을 이용

- 시계열 데이터
  - 시간 또는 주기를 가지고 발생하는 데이터
  - 그라파나에서는 히스토그램을 이용해서 표현하는 경우가 많습니다.
  - Loki는 히스토그램과 유사한 차트를 사용하는데 이를 통해서 특정 시간 동안의 분포를 이해할 수 있고 Gantt 차트로 전환할 수 있습니다.
  - 프로메테우스에서는 Exemplar 가 추적에 대한 근사치를 시계열로 표현
  - 시계열 데이터를 표현할 때 한쪽에는 메트릭을 그리고 다른 한쪽에는 로그를 출력해서 상관 관계를 표현하는 경우가 있음   
    상관 관계를 표현하면 복잡성을 낮추고 이해도를 높여줍니다.   
    통계적으로 계산하고 상관 계수를 구하기도 합니다.   
    회귀분석을 이용해서 통계적인 상관 관계를 구하는데 자주 사용하는 방법 중 하나가 됩니다.   
    상관 계수를 직접 구하지 않고 빠르게 변수 들 간의 상관 관계를 파악하고자 할 때는 Heatmap을 사용하기도 합니다.   
    통계와 머신러닝을 이용해서 상관 관계를 수치화하고 자세히 분석하는 것은 중요한 작업이지만 신속하게 판단하고자 할 때는 시각적인 방법인 히스토그램이나 히트맵을 사용하기도 합니다.
- 메트릭 관리 방안
  - 메트릭은 단순히 집계 모니터링에 그치지 않고 오토스케일링 등 다양한 분야와 연계가 되므로 여러가지 추가적인 개발과 구성을 필요로 함
  - 운영환경에서는 기본 메트릭만으로는 부족하고 복잡한 커스텀 매트릭을 이용해서 쿠버네티스 오토스케일링을 수행할 수 있음
  - 메트릭을 이용한 대시보드를 만들 때 주의할 점   
    한 번에 너무 많은 정보가 전달돼서 추론하기 어렵지 않도록 설계

    최상위 수준의 대시보드는 하나만 만들어서 서비스가 제대로 작동하는지 한 눈에 보여주어야 함

    메트릭을 표시할 때 응답 시간, 에러, 트래픽 같은 가장 중요한 것에 집중해야 함

    가능하면 태그를 사용해 매트릭에 상황 정보를 표현

    메트릭의 이름을 만들 때 정의된 표준을 지켜야 한다.   
    일반적으로 `서비스이름.메서드이름.유형`의 형태

### 3)go를 이용해서 프로메테우스 매트릭을 출력
- EC2 인스턴스 접속

- git 설치
```
sudo apt install git
```
- 데이터 가져오기
```
git clone https://github.com/itggangpae/prometheus.git
```
- golang 설치
  - 다운로드: `wget https://golang.org/dl/go1.23.4.linux-amd64.tar.gz`

  - 압축풀기: `sudo tar -C /usr/local -xzf go1.23.4.linux-amd64.tar.gz`

  - 환경 변수 설정: `nano .profile`
```bash
export GOROOT=/usr/local/go
export GOPATH=$HOME/go
export PATH=$GOPATH/bin:$GOROOT/bin:$PATH
```
  - 수정한 내용 적용: `source ./.profile`

- main.go 파일 실행
  - 모듈 생성:  
  ```
  go mod init 이름
  go mod tidy
  ```
  - 빌드: `go build`
  - 실행: `go run  main`

- 애플리케이션은 8080 포트로 매트릭을 내보냄
  - 리눅스에서 확인: `curl -H 'Accept: application/openmetrics-text' localhost:8080/metrics`
  - 브라우저 확인: EC2 인스턴스에서 8080 포트를 개방한 후 publicIP:8080 으로 접속

### 4)Tracing
- 추적 구성 요소
  - 추적과 가장 유사한 용어는 APM
  - APM 특징   
    자동화 된 계측을 선호

    로그와 매트릭보다는 성능 관리에 집중하기 때문에 상관관계의 기능이 부족

    매트릭은 수집 정보에서 자동으로 계산하는 경우가 대부분이고 매트릭의 추가는 어려움
  - 관측 가능성은 기존 모니터링의 영역을 포함하고 자동화와 데이터 분석 기능을 추가해서 다양한 영역으로 확장이 가능
  - 관측 가능성의 구현 과정은 클라우드와 오픈 소스를 사용해 클라우드 네이티브에 적합한 관측 가능성 환경을 만들어 가는 것
  - 개발자, 운영자, 데이터 엔지니어, 데이터 분석가가 원하는 완전한 기능을 구현한 솔루션을 구입하는 것이 어렵고 조직 간 구성원의 다양한 요구사항을 만족시키는 것 또한 쉽지 않기 때문에 점진적, 지속적인 개선을 통해서 조직에 적합하도록 보완해 나가는 것이 오픈 소스와 클라우드 네이티브에 기반한 관측 가능성
- 추적에 관련된 용어
  - span   
    전반적인 수행 시간 뿐만 아니라 각기 하위 동작의 시간과 소요 시간 정보를 알 수 있는 것

    동작 정보, 타임 라인과 부모 스팬과의 의존성을 포함하는 수행 시간을 담고 있음

    태그, 로그, 스팬 컨텍스트, 배기지 정보를 포함하고 있음
  - span context(문맥 - 정보를 저장하는 구조)   
    새로운 스팬을 생성하기 위해서는 다른 스팬을 참조해야 하는데 이 때 스팬 컨텍스트는 필요한 정보를 제공

    스팬 컨텍스트로 표현되는 메타 데이터를 쓸 수 있는 inject와 읽을 수 있는 extract 메서드를 제공

    스팬 컨텍스트는 inject와 extract를 통해 헤더에 전달되고 전달된 내용 중 스팬 정보로 자식 스팬을 생성할 수 있음

    하나의 서비스를 호출하면 하나의 스팬이 생성되고 이 서비스에서 다른 서비스를 호출하면 새로운 스팬이 생성될 때 이전 스팬의 정보를 스팬 컨텍스트를 이용해서 제공받음
  - 스팬 레퍼런스   
    스팬 사이의 인과 관계   

    child of 와 following from 관계
- 예거를 이용한 추적 실습
  - 도커를 이용해서 예거 설치   
    `sudo apt install -y docker.io`
  - Jaeger   
    Uber에서 개발한 분산 서비스가 트랜잭션을 추적하는 오픈 소스 소프트웨어   

    MSA 환경을 모니터링하는 프로젝트

    다양한 마이크로 서비스의 요청 경로를 추적하고 요청 플로우를 시각적으로 확인하고 분산 트랜잭션을 모니터링 할 수 있음   
    이를 기반으로 대기 시간과 성능을 최적화할 수 있음

    - 기본 구조
    Jaeger Client: OpenTracing API로 만들어진 언어별 구현체로 CNCF 산하의 프로젝트로 애플리케이션 간 분산 추적을 위한 표준처럼 사용되는 비공식 API인데 현재는 OpenTelemetry SDK로 대체 중

    OpenTelemetry SDK: OpenTelemetry는 응용 프로그램이 프로세스에서 다양한 원격 측정 데이터를 매트릭 및 추적 백엔드로 보내는 것이 목표이고 Jaeger는 추적 Telemetry 데이터를 수신해서 해당 데이터의 처리, 집약, 데이터 마이닝, 시각화를 제공하는 백엔드

    Jaeger Agent: UDP를 통해 전송된 Span을 수신하는 네트워크 데몬으로 처리한 후 Collector로 trace를 전송

    Jaeger Collector: Agent로 부터 Trace를 수신해서 파이프라인을 통해 유효성 검사, index 생성 및 변환을 수행한 후 DB에 저장

    Jaeger Query: DB에서 trace를 조회한 후 UI 구성에 필요한 API를 제공

    Storage(DB): 추천되는 데이터베이스는 ElasticSearch와 Cassandra 이지만 호환성이 검증된 Promscale 이나 TimescaleDB(PostgreSQL 기반의 time-series DB), ClickHouse(컬럼 기반의 RDBMS) 와도 호환되고 ScyllaDB, InfluxDB, Amazon의 Dynamo DB 등에 대한 지원도 진행 중

    App <-> Jaeger-Agent -> Jaeger-Collector -> Kafka -> Jaeger Ingester, Flink Streaming -> Storage -> Jaeger-Query -> Jaeger UI
  - 도커에 설치   
    ```bash
    sudo docker run -d --name jaeger -e COLLECTOR_ZIPKIN_HOST_PORT=9411 -e COLLECTOR_OTLP_ENABLED=true -p 6831:6831/udp -p 6832:6832/udp -p 5778:5778 -p 16686:16686 -p 4317:4317 -p 4318:4318 -p 14250:14250 -p 14268:14268 -p 14269:14269 -p 9411:9411 jaegertracing/all-in-one:1.42
    ```
  - 확인: 브라우저에서 16686 포트로 접속
  - 샘플 코드   
    다운로드: git clone https://github.com/jaegertracing/jaeger

    실행을 하고 8080 포트로 접속을 하면 4가지 버튼이 있는 웹 서비스가 실행되는데 애플리케이션   
	여러 마이크로 서비스가 별도 port 로 실행되는 간단한 MSA 환경   
	8080 에 frontend, 8081에 customer, 8082에 driver 그리고 8083에 route 라는 서비스가 실행디는 구조   
	이 프로그램은 go로 작성   
	spring boot 로 만들고자 할 때는 Jaeger Dependency를 이용해서 로그를 작성   

	실행은 go run ./main.go all
- 샘플 코드 다운로드
```bash
git clone https://github.com/itggangpae/prometheus.git

# 요청을 전송할 클라이언트 생성
cd prometheus/cd jaegar
cd client
go mod init hello
go mod tidy
go build

# hello-world client가 호출할 웹 서비스 생성
cd ../formatter
go mod init formatter
go mod tidy
go build

cd ../publisher
go mod init publisher
go mod tidy
go build

# 실행
cd ../formatter
./formatter

cd ../publisher
./publisher

cd ../client
./client 매개변수 매개변수
```
- jaeger UI 화면에서 새로고침을 하고 services에서 선택

### 5)로그
- 로그 관리
  - 로그 관리의 일차적인 목적은 시스템에 분산된 로그를 한 곳에 저장해서 다루기 쉽게 하는 것
  - 로그를 중앙 집중적으로 수집하고 관리할 수 있는 인프라와 로그 관리 시스템을 구축한 상태에서는 다양한 로그 유형을 **표준화**하는 작업이 필요
  - 로그 유형이 제각각이면 로그 내용을 쉽게 검색하는 것이 어렵고 후속 처리(전처리)를 위한 가공과 집계에 많은 시간이 걸림
  - 로그 라이브러리 사용하기 -> 동일한 포맷의 로그 출력, 전처리 필요성 감소
  - 로그 관리 시스템의 목적은 모든 서비스에서 로그 데이터를 수집해서 필요할 때 시스템의 동작을 추론하고 검색하는 것인데 이를 활용해서 검사, 디버그, 새로운 통찰을 얻을 수 있음
  - 로그 데이터를 효과적으로 저장하고 검색하려면 우선 엔지니어링 팀이 사용할 로그의 형식을 합의하고 이를 표준화해야 합니다.
- 로그 관리에서 고려할 사항
  - 서비스가 재시작되거나 예상치 못한 장애가 발생할 수 있는데 이러한 상황을 극복하고 지속적으로 로그를 수집, 관리해야 합니다.
  - 급격하게 증가하는 로그 데이터를 처리하기 위해 로그 관리 시스템은 동적이고 수평적인 확장이 가능해야 합니다.   
    가상 머신, 도커 컨테이너, 쿠버네티스 파드 등 다양한 런타임에서 동일한 방식으로 로그 데이터를 수집할 수 있어야 합니다.
  - 클라우드 네이티브는 멀티 테넌트를 지원해야 합니다.   
    단일의 로그 시스템을 이용해서 다양한 조직 과 서비스에서 생성되는 로그 데이터를  수집하고 관리할 수 있어야 합니다.
  - 수집된 데이터를 활용해서 검색 과 후속 처리를 할 수 있어야 합니다.
- 로그 데이터 의 예
  - 프론트 앤드 와 백 앤드 애플리케이션에서 수집한 성능 데이터
  - 프록시, 로드 밸런서 등에서 수집한 네트워크 트래픽 로그
  - 웹 서버, 서버 프레임워크, 언어 별 로깅 라이브러리에서 수집한 데이터
  - SAP 등 벤더 솔루션 패키지 와 레거시 시스템에서 수집한 비구조적인 로그: 자체 고유한 형식의 로그를 기록하기 때문에 형식을 제어할 수 없는데 그 특수성에 대응하고 어떻게든 변환을 수행
- 로그 데이터의 포함할 정보
  - 타임스탬프   
    데이터가 상호적으로 연관되고 적절한 순서를 갖도록 정하려면 로그 엔트리에 타임스탬프를 포함해야 함   
    각 서비스는 자체 타임스탬프를 표현해야 하는데 최근에는 마이크로 세컨드 단위로 기록하는 것을 권장함   
    시간대 정보를 포함시키는 것을 권장하며 되도록이면 GMT/UTC로 사용하는 것을 권장   
    발생 시간으로 데이터를 정렬하면 분석하기 쉽고 적은 문맥정보가 필요
  - 식별자   
    저장하려는 데이터에 가능한 많은 고유 식별자를 포함시켜야 함   
    주문 ID, 트랜잭션 ID 등 다른 고유 식별자는 여러 소스 데이터를 상호 참조할 때 중요한 가치가 됨
  - 소스   
    로그 엔트리의 소스를 식별하면 쉽게 디버깅할 수 있음   
    소스 데이터는 호스트, 클래스, 모듈, 함수, 파일명 등   
    이 데이터와 실행 시간이 같이 추가하면 나중에 소스에 수집된 정보에서 성능을 계산할 수 있습니다.   
    매트릭을 대체하지는 않지만 병목 구간 과 잠재적 성능 문제를 식별하는데 효과적
  - 레벨 또는 카테고리   
    각 로그 엔트리는 카테고리를 포함해야 합니다.   
    카테고리는 로그 데이터의 유형이거나 로그 레벨   
    일반적으로 로그 레벨에는 ERROR, DEBUG, INFO, WARN 값이 사용됩니다.   
    레벨이 있다면 로그 관리 시스템은 ERROR 레벨을 가진 메시지를 찾아내서 에러 보고 알람 등을 생성해서 운영자에게 전송할 수 있습니다.   
    ElasticSearch와 Loki에서는 이러한 기능을 쉽게 개발할 수 있음
- 로그 엔트리를 생성할 때 사람이 읽을 수 있는 형태인 동시에 머신에 의해서 쉽게 파싱할 수 있어야 하므로 JSON 형식을 이용하는 것을 권장
- 로그 엔트리를 만들 때 여러 줄에 걸친 로그를 피해야 하는데 집계도구에서 파싱할 때 파편화를 초래할 수 있기 때문임
- 로그 표준화
  - 오픈 텔레매트리에서 소개하는 로깅 파이프라인은 여러 모로 의미있는 작업
  - 오픈 텔레메트리는 로깅 인터페이스 표준화와는 관계가 없음
  - 오픈텔레매트리에서 제공해주는 기능   
    LogRecord 데이터를 생성하는 LogEmitter

    언어별 로깅 라이브러리 와 통합하고 연계하는 방법

    구조화 된 로그를 생성하는 방법

    Emitter 와 Log Level을 사용해서 최적화되고 필요한 로그만 출력하는 방법

    리소스, 추적 등 상관 관계 데이터를 로그에 추가하는 방법

    다양한 오픈텔레매트리 에이전트 와 연계하는 방법
- go 언어에서 구조화된 로그 출력
  - go에서는 uber에서 만든 zap 라이브러리가 구조화된 로그를 생성할 수 있음
  - key - value 구조로 대입해서 json 형식으로 출력
```
{
	“level”: “info”, 
	“msg”: “failed to fetch URL”,
	"url": "https://github.com"

}
```
위와 같은 형식으로 출력이 됩니다.
### 6)상관관계
- 상관관계의 필요성
  - 데이터 분석가들은 서로 다른 데이터가 정확하게 결합이 되어 있을때 비로서 데이터의 활용 가치가 증가한다라고 함
  - 로그, 매트릭, 추적, 트래픽 데이터는 서로 다른 이기종 데이터가 아니고 아직 결합되지 않은 분리 데이터 뿐
  - 관측 가능성의 활용도를 높이고 새로운 가치를 재발견하기 위해서는 데이터간의 결합이 필요
  - 로그에 대한 통계와 집계 정보는 매트릭이 될 수 있고 로그를 발생 시점 과 처리 시점을 계산해서 나열하면 추적이 될 수 있습니다.
  - 관측 가능성의 세가지 요소를 관리하기 위한 시스템은 개별적으로 구축한 후 관리하는 것이 일반적이지만 보다 효율적이고 성공적인 결과를 얻기 위해서는 모두가 하나의 통합 플랫폼으로 연결이 되어야 하는데 이 통합을 위해서 세 가지 요소간 상관 관계를 정의해야 함

### 7)관측 가능성 데모
- 관측 가능성 에서의 계측은 API 와 SDK를 사용해서 추적, 로그, 매트릭에 대한 신호를 정의하고 수집하는 작업   
  오픈텔레매트리의 경우에는 자동 계측과 수동 계측을 지원   
  수동 계측: 직접 소스 내 API를 구현하는 방법   
  자동 계측: 소스의 수정없이 외부에서 계측을 위한 데몬을 통해 간접적으로 계측을 수집
- 자바는 가상 머신의 성숙하고 우수한 프로파일 기술과 J2EE 스프링 프레임워크의 유연함을 바탕으로 백엔드 영역에서 가장 많이 사용하는 언어   
  자동 계측은 자바 가상 머신 기반으로 상세한 내부 데이터를 제공하며 개발자의 추가적인 개발없이도 단기간 내에 우수한 모니터링 시스템의 구축을 실현해줍니다.   
  마이크로서비스로 구현되는 클라우드 네이티브 와 쿠버네티스 컨테이너 환경에서 자바는 무거운 자바 가상 머신 과 복잡한 라이브러리 참조를 수행하기 때문에 경량화된 컨테이너 사상과 부합하지 않습니다.

  파이썬은 데이터 분석 그리고 Go는 동시성 구현에 유리하다는 장점이 있는데 대규모 백엔드에는 이 둘이 더 적합할 수 있다고 합니다.   
  이 둘의 언어는 자바 가상 머신처럼 low level의 프로파일 정보를 제공해주지는 않습니다.

  다양한 언어를 지원할 수 있도록 API를 제공해주는 것은 어려운 작업인데 표준화된 단체 아래 다양한 벤더들이 공동으로 작업을 했는데 그 결과물이 오픈텔레메트리

  자동계측에는 언어별 에이전트가 필요한데 자바와 파이썬의 경우는 완성도가 높고 Go는 2개 언어에 비해서는 미흡하고 Spring Boot, Flask, Kafka, Redis 등 오픈 소스도 완성도에 차이가 있음
- 대다수 프로그램의 구조   
  Application->Collector->관측 가능성
  - Application은 가급적이면 마이크로서비스 형태로 구현하고 REST를 사용하고 Stateless하게 구현하는 것을 궈장
  - 백엔드를 개발할 때는 내부적으로 메트릭, 로그, 추적 등을 생성하고 외부로 노출 혹은 전송
  - 컬렉터를 구현할 때는 오픈텔레메트리를 사용하는 경우에는 오픈텔레메트리 컬렉터를 사용하고 그라파나 로키를 사용하면 Promtail, 프로메테우스 메트릭을 사용하면 exporter, 예거를 사용하면 오픈트레이싱 API, 그라파나를 사용하는 경우는 그라파나 에이전트가 있음
  - 관측 가능성도 그라파나의 Loki 나 Tempo 또는 프로메테우스의 mimir 등 이 사용됩니다.
  - 애플리케이션은 되도록이면 쿠버네티스 파드로 배포
- 위브윅스의 TNS나 AWS의 o11y Shop 등이 제공

### 7)관측 가능성 오픈 소스
- OpenTelemetry
  - 계측을 위한 표준화된 도구, API, SDK의 모음
- Loki
  - 그라파나의 로그 관리 시스템
- Grafana Mimir: 프로메테우스용 오픈 소스
- Thanos: 다수의 프로메테우스를 통합할 수 있는 글로벌 뷰를 제공

- 예거
  - 분산 컨텍스트 전파를 포함한 마이크로서비스 기반 분산 시스템 모니터링 과 추적을 위해서 사용
  - 다양한 백엔드 저장소와 통합이 가능하고 상세한 근본 원인 분석이 가능

- Grafana Tempo: 그라파나의 오픈소스 추적 솔루션

## 2.관측 가능성 기반 기술
### 1)트래픽 관리
- 단일 장애점
  - 분산 클라우드 네이티브 환경에서 치명적인 장애는 대부분 네트워크와 관련된 장애
  - 네트워크 장애는 고객 서비스 이용이 불가능한 상황을 발생시킴
  - 네트워크 데이터 중에서 로드 밸런서, 웹 서버 액세스 로그, 보안 접근 제어 데이터 등은 중요하게 관리
  - 쿠버네티스 네트워크의 구성은 복잡: DNS(KubeDNS), Proxy(KubeProxy), Gateway를 내부적으로 관리하고 각 구성 요소에 문제가 생기면 전체 클러스터 장애로 전파됨
  - 분산 마이크로서비스와 동적인 클라우드 환경에서는 관측 가능성만으로는 장애를 극복하기에 충분하지 않기 때문에 네트워크 트래픽 데이터를 수집하고 관리를 해야 함
- Load Balancer
  - 복원성이 뛰어나고 탄력적인 네트워크를 구축하기 위한 필수 요소
  - AWS 기반의 네트워크 흐름   
    외부에서 유입되는 트래픽은 AWS NAT Gateway를 통해서 VPC로 유입됩니다.   
    VPC 내부는 다수의 서브넷으로 구성   
    트래픽은 서브넷에서 AWS ELB를 경유하고 쿠버네티스 인그레스로 유입되고 쿠버네티스 인그레스의 경로에 맞게 특정 애플리케이션으로 라우팅 됩니다.
  - 쿠버네티스는 자체적인 로드밸런서와 DNS 서버를 가지고 있으며 CoreDNS를 통해서 서비스를 찾아내고 KubeProxy를 이용해서 특정 파드로 로드밸런싱을 수행
  - 로드밸런싱 종류   
    플랫폼 로드 밸런싱   
    AWS ELB, Nginx가 플랫폼 로드밸런서에 해당   
    온프레미스 환경에서는 IIS, 엔진엑스, 아파치 등을 이용해서 정적인 로드 밸런싱 기법이 많이 사용됨   
	그라파나의 관측 가능성은 엔진엑스를 이용해서 플랫폼 로드 밸런서를 구현하며 public cloud에서 사용하는 경우에는 엔진엑스 앞에 AWS ELB를 사용   
	그라파나의 관측 가능성을 사용할 때는 ConfigMap을 사용해서 엔진엑스에 엔진엑스 리버스 프록시 구성 파일을 생성해야 합니다

    게이트웨이 로드 밸런싱   
    소프트웨어 기반 게이트웨이는 오픈소스로 쉽게 구현 가능   
	스프링 클라우드 기반의 게이트웨이, 넷플릭스에서 만든 zuul 이 대표적
	일반적인 용어로는 API Gateway 라고 합니다.   
	플랫폼의 트래픽 로드 밸런싱은 가용성 최적화 측면에서 한계를 보이게 됨   
	레이턴시 유형의 가용성 정보를 정확히 관찰하지만 리소스 사용률 유형의 신호를 측정할 때는 서버가 가장 정확한 정보를 제공   
    API Gateway는 기본적인 Routing 뿐 아니라 A/B 테스트, 카나리아 배포를 제공

    클라이언트 측 부하 분산   
	서비스에서 애플리케이션 코드로 부하를 분산하는 것
- 복원성 패턴
  - 마이크로서비스는 일반적으로 분산 시스템 복원력을 높이기 위해서 여러 가용 영역에 수평 확장이 가능하도록 배포
  - 서비스 메시(애플리케이션의 서비스 간 모든 통신을 처리하는 계층을 만드는 것으로 Istio를 이용해서 구현하는 경우가 많음)를 사용하면 네트워크 구간의 모니터링이 가능하고 복원성 패턴을 구현할 수 있음
  - Istio 서비스 메시를 사용하면 관측 가능성과 연계할 수 있음   
    Istio 서비스 메시가 수집한 데이터를 로그로 활용
  - 재시도   
    에러의 유형은 일반적으로 비지니스 에러 와 시스템 에러로 나눌 수 있는데 비지니스 에러는 호출자가 잘못된 데이터를 입력하거나 프로그램의 버그로 인해서 발생하는데 이 경우 재시도는 불필요   
    시스템 에러는 네트워크의 일시적인 지연이나 서버 와 백엔드의 장 등이며 이런 경우는 일정 주기도 재시도를 하면 정상적인 처리가 가능   
    재시도는 가끔 발생하는 장애에 대응하는 효율적인 수단이지만 최초 요청이 실패했을 때 부터 재시도 압력이 누적돼 시스템에 과부하를 일으키면 안되므로 반복적이고 무차별적인 재시도에 주의해야 합니다.

    다운스트림 서비스는 순간적인 장애를 자주 겪습니다.   
	thread pool 포화, 네트워크 접속 지연으로 시간 초과 등 가용성을 저해하는 요인이 많기 때문에 단시간 내에 자동으로 비활성화되며 호출자는 일시적 오류에 대처해야 합니다.

  - 비율 제한
  - 벌크헤드   
    마이크로 서비스 여러 개가 연관되서 동작하는 경우 하나의 서비스의 가용성이 저하되면 다른 서비스도 가용성이 저하될 수 있습니다.

    bulkhead 패턴은 다운스트림 서비스를 서로 격리하고 각 서비스의 동시 처리 능력을 제한하는 것
  - Circuit Breaker   
    벌크 헤드가 확장된 개념으로 임계 비율을 설정해서 임계 비율 이상이 되면 다른 서비스로 전환하는 패턴   
    넷플릭스의 추천 영화 목록은 서킷 브레이커 패턴을 적용한 대표적인 기능인데 추천 목록은 가입자의 과거 시청 기록 등을 기반으로 개개인의 독립적인 선호도를 계산해서 선정하는데 너무 트래픽이 몰리면 추천 컨텐츠 목록 대신에 일반 컨텐츠 목록으로 응답